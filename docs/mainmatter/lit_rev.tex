\documentclass[../main]{subfiles}

\begin{document}
\chapter{Background}
\label{chap:background}
Breast cancer screening may use multiple imaging modalities, including X-ray mammography, digital ultrasound, and hispathological microscopy, each offering different diagnostic information. Deep convolutional neural networks (CNNs) have proven to be remarkably effective across these modalities, by learning complex image features from a given dataset \autocite{jiang2024deep} \autocite{carriero2024deep}. Widely-used mammography datasets include the Digital Database for Screening Mammography (DDSM) \autocite{carriero2024deep} and the Curated Breast Imaging Subset of Digital Database for Screening Mammography (CBIS-DDSM) containing around 10,000 images \autocite{carriero2024deep}, and the INbreast dataset containing around 400 FFDM (full-field digital mammography) images \autocite{carriero2024deep}. For histopathology, the BreakHis dataset, containing about 9,000 breast tissue images at 40x-400x magnification, and BACH (Breast Cancer Histology) dataset are commonly used as benchmarks \autocite{jiang2024deep} \autocite{srikantamurthy2023classification}. For ultrasound imaging, the BUSI (Breast Ultrasound Image) dataset, has also been used in CNN studies \autocite{latha2024revolutionizing}. Such public repositories have enabled various CNN-based breast cancer detection and classification studies.

\section{History}
\label{sec:history}
Computer-aided detection (CAD) systems for breast cancer originated in the 1980s with the advent of digital mammography, marking the beginning of using computer systems to assist radiologists. By the mid-1980s, major strides were made in developing algorithms for use in mammography (and also chest X-rays) \autocite{https://doi.org/10.1118/1.3013555}. These early CAD systems used hand-engineered image processing, and statistical classifiers to detection calcifications or masses. In the 1990s, the first studies involving neural networks for CAD appeared. These involved the application of the first convolutional neural network to mammographic microcalcification detection \autocite{https://doi.org/10.1118/1.3013555}. In subsequent years, the use of more traditional machine learning (ML) methods like support vector machines (SVMs), and decision trees were also applied to breast pathology, and imaging, using handcrafted features like texture, shape, and histogram statistics to classify regions as benign or malignant. Nowadays, CNNs are dominant in both mammography, and histopathology tasks, without using classical pipelines that extract features for a separate classifier. The use of CNNs allows for the direct learning of features from pixels, allowing for better accuracy, and generalisation \autocite{araujo2017classification}. Namely, studies have shown that CNN-extracted features used in an SVM achieved a 95.6\% sensitivity on histology slides \autocite{araujo2017classification}. In general, CNN-based models outperform traditional feature-based SVMs or random forest classifiers across many breast image datasets, though SVMs, and trees are still used for certain tasks such as radiomics-based subtype classification \autocite{guo2024machine}. The disadvantages of using CNNs usually include the necessity of large, labelled datasets, and heavy computational resources for their training, and inference. This however comes at the advantage of automated feature learning, and the identification of subtle, complex patterns that classical machine learning models struggle to come to terms with. On the other hand, SVMs and decision trees can work with smaller datasets, but depend on domain-expert feature design, and may not generalise as well to unseen data \autocite{araujo2017classification}.

\section{Convolutional Neural Network Architectures, and Methods}
\label{sec:cnn-arch}
Modern CNN-based architectures for breast cancer detection are typically reliant on established architectures (like ResNet, Inception, VGG, DenseNet, and EfficientNet) designed for medical imaging. Transfer learning is a common approach, where pre-trained models are fine-tuned on medical scans to compensate for limited annotated data \autocite{srikantamurthy2023classification}. This approach has been shown to outperform using CNNs as fixed feature extractors. Data augmentation like flips, rotations, colour jittering, and GAN-based augmentation is also commonly used, particularly in ultrasound, and pathology where datasets are limited \autocite{latha2024revolutionizing}. Namely, Gupta et al. state that the random flipping, and rotation of ultrasound images, helps to overcome class imbalance on the BUSI dataset \autocite{latha2024revolutionizing}. Explainability methods like Grad-CAM heatmaps are incorporated to visualise salient regions, and build trust in the model \autocite{latha2024revolutionizing} \autocite{kaba2024explainable}. In segmentation tasks (like in cases of lesions), encoder-decoder networks like U-Net remain the industry standard, yielding critical performance on medical image segmentation tasks \autocite{jiang2024deep}.

\section{Mammography-based detection}
\label{sec:mammography-cnn}
CNNs have demonstrated a high diagnostic accuracy by classifying whole images or localised patches of mammograms. Studies often use the DDSM or INbreast dataset, and the Digital Breast Tomosynthesis (DBT) dataset. Namely, a commercial deep CNN-based system, trained on about 1,000 mammograms reached an AUC score of 0.82, comparable to an expert radiologist \autocite{carriero2024deep}. Retrospective evaluations indicate the Computer Aided Diagnosis (CAD) systems powered by AI can improve radiologist sensitivity, such that their cancer detection rate rose from 51\% to 62\% after the introduction of AI aid as evidenced by Watanabe et al. \autocite{carriero2024deep}. In a similar study, Kim et al. reports that an unassisted AI-CAD system achieved an AUC of 0.94, outperforming the radiologist's AUC of 0.88 \autocite{carriero2024deep}. Akselrod-Ballin et al. achieved an AUC of 0.91 using a large linked dataset of mammograms, and patient records \autocite{carriero2024deep}. In large screening challenges (with around 85,000 training images, and 68,000 external images), top CNN ensembles reached an AUC of 0.90 on held-out mammograms \autocite{carriero2024deep}. Although still below expert performance, combining radiologists with AI yielded an AUC of 0.94, indicating the performance gains experienced when the two are synergised. CNNs have also been applied to lesion detection, and segmentation, with a particular case in which a U-Net-like model, trained on a CBIS-DDSM dataset, detected masses with 95.7\% sensitivity, and a Dice score of 74.5\% \autocite{jiang2024deep}.

Popular CNN models for mammography include ResNet50, VGG16/19, Inception, and EfficientNet. For tasks like the detection of mass classification or microcalcification detection, such networks often outperform more traditional, feature-based methods \autocite{wang2024mammography} \autocite{carriero2024deep}. Namely, Shen et al. found a fine-tuned CNN achieved an 88\% accuracy whilst classifying mammographic tumourss, outperforming radiologists which achieved 83\% accuracy \autocite{wang2024mammography}. In another study, Yala et al. reports CNN risk models with an AUC of 0.84 against an AUC of 0.77 for radiologists \autocite{wang2024mammography}. Object detection models like Faster-RCNN, and RetinaNet were also used within the Agarwal et al. study, which achieved a 99\% accuracy for malignant-mass detection on the INbreast dataset, when applying Faster-RCNN to full-field mammograms \autocite{jiang2024deep}. More recent works, use transformer-based models using a pretrained vision transformer, and segmentation, which reaches a 99.96\% accuracy on INbreast, by first extracting a mass ROI \autocite{kumar2024segmentation}. All in all, CNNs on mammograms achieve a high AUC value, often similar to or exceeding the performance of an average radiologist \autocite{carriero2024deep}.

\section{Histopathology-based detection}
\label{sec:histopathology-cnn}
CNNs have also been applied to histopathology slides for cancer classification. The BreakHis dataset (containing microscopic tissue patches at 40x-400x magnification) has been the de facto benchmark for this. Basic CNNs like ResNet, and VGG achieve more than 90\% accuracy on binary classification tasks upon the BreakHis dataset (when classifying benign against malignant cancers) \autocite{jiang2024deep}. In Han et al. a CNN was applied to the BreakHis dataset, achieving a 93.2\% accuracy \autocite{jiang2024deep}. More complex architectures can improve performance significantly more, such as in Munikoti et al. which combines a CNN with an LSTM (utilising ImageNet transfer learning), and achieving a 99\% accuracy on binary classification in the BreakHis dataset \autocite{srikantamurthy2023classification}. Such deep learning models significantly outperform more traditional machine learning methods like Rao et al. which found that CNNs (with transfer learning) superseded SVMs or random forests on histology data \autocite{yusoff2023accuracy}. Apart from patch classification, CNNs like U-net are used to segment nuclei or tumor regions to help analysis \autocite{jiang2024deep}. Modern approaches use attention or multiscale to capture complex tissue contexts \autocite{srikantamurthy2023classification}. In conclusion, CNNs on histopathology often achieve very high accuracies of 90-99\% on curated datasets \autocite{srikantamurthy2023classification} \autocite{yusoff2023accuracy}, taking advantage of deep architectures, and large-scale patch augmentation.

\section{Ultrasound-based detection}
\label{sec:ultrasound-cnn}
When breast tissue is dense, ultrasound imaging is often used as an imaging modality. The use of this, posits issues relating to speckle noise, and operator variability. CNNs have shown remarkable results despite this. Since public ultrasound datasets are smaller, transfer learning from networks like VGG-16, VGG-19, and AlexNet are often used. Wang et al. applied a multi-view CNN with ImageNet pretraining to a breast ultrasound, and achieved an AUC of 0.9468 \autocite{wang2024mammography}. In another study, Gupta et al. fine-tuned EfficientNet-B7 on the BUSI dataset, and reported a 99.1\% accuracy \autocite{latha2024revolutionizing}. Augmentation methods like rotations, and colour jitter were used, and Grad-CAM explanation was used to focus on lesion features \autocite{latha2024revolutionizing}. In addition, Rahman et al. deduced that a fine-tuned model of InceptionV3 outperformed VGG-19 for ultrasound tumour classification \autocite{ayana2021transfer}. In general, ultrasound CNNs commonly exceed 90\% accuracy, with transfer learning, and data augmentation necessary to compensate for limited sample sizes.

\section{Transfer Learning, Data Augmentation, and Explainability}
\label{sec:transfer-learning}
Since annotated breast imaging data is limited, transfer learning, and augmentation are essential. Almost all recent studies initialise CNNs with ImageNet weights, and fine-tune on medical images \autocite{srikantamurthy2023classification}. Fine-tuning routinely outperforms training from scratch or using CNNs as pure feature extractors. Strategies for augmentation, including geometric transforms, colour perturbations, and GAN-based synthesis help to ease overfitting problems. Namely, EfficientNet-based classifiers for ultrasound used random flipping, rotation, and colour jitter to improve minority malignant cases \autocite{latha2024revolutionizing}. Other histology studies have also used flips, rotations, and staining augmentation to improve the diversity of tissue patches \autocite{srikantamurthy2023classification}.

Explainable AI is becoming increasingly important in medical imaging, where methods such as heatmaps (like Grad-CAM, and saliency maps) are applied to highlight image regions to make predictions \autocite{latha2024revolutionizing} \autocite{kaba2024explainable}. This helps to verify that CNNs focus on tumours rather than artifacts. Studies report overlaying class activation maps on mammographs or ultrasound to show ROI localisation. Namely, EfficientNet-B7 visualised Grad-CAM heatmaps on ultrasound to confirm lesion focus \autocite{latha2024revolutionizing}. Explainability is also explored via feature attribution, and partial dependence, but Grad-CAM remains popular \autocite{latha2024revolutionizing} \autocite{kaba2024explainable}.

\section{Performance vs. Traditional Methods, and Radiologists}
\label{sec:performance}
CNN-based systems have been shown to outperform classical machine learning algorithms, and approximate expert-level analysis. Deep models have reduced false positives relative to older CAD systems, and improved sensitivity. On mammography tasks, CNNs matched or superseded radiologist sensitivity as demonstrated by Becker et al., which reported an equal AUC of 0.82 for a commercial CNN, and human experts \autocite{carriero2024deep}. In larger trials, deep learning models achieved an AUC of 0.86 to 0.94, while experts scored slightly better, but, when combining AI with human experts, recall rates were consistently reduced \autocite{carriero2024deep}. In ultrasound, CNNs have outperformed conventional classifiers like SVMs, and decision trees when given enough data \autocite{yusoff2023accuracy}. In histopathology, CNNs significantly outperform custom feature methods, where hybrid CNN and TL models reached an accuracy of 97\% on BreakHis, and classical machine learning models rarely exceeded 90\% on the same task \autocite{yusoff2023accuracy}. Thus, studies indicate that CNNs offer excellent sensitivity, and specificity, across different modalities \autocite{carriero2024deep} \autocite{wang2024mammography}, demonstrating their practical utility within CAD systems.

\end{document}
