\documentclass[../main]{subfiles}

\begin{document}
\chapter{Background}
\label{chap:background}
Breast cancer screening may utilise multiple imaging modalities, including X-ray mammography, digital ultrasound, and histopathological microscopy, each providing distinct diagnostic information. Deep convolutional neural networks (CNNs) have proven remarkably effective across these modalities by learning complex image features from a given data set \autocite{jiang2024deep} \autocite{carriero2024deep}. Widely used mammography data sets include the Digital Database for Screening Mammography (DDSM) \autocite{carriero2024deep} and the Curated Breast Imaging Subset of the Digital Database for Screening Mammography (CBIS-DDSM) containing around 10,000 images \autocite{carriero2024deep}, and the INbreast data set containing around 400 FFDM (full-field digital mammography) images \autocite{carriero2024deep}. For histopathology, the BreakHis data set, containing about 9,000 breast tissue images at 40x-400x magnification, and the BACH data set (Breast Cancer Histology) are commonly used as benchmarks \autocite{jiang2024deep} \autocite{srikantamurthy2023classification}. For ultrasound imaging, the BUSI (Breast Ultrasound Image) data set has also been used in CNN studies \autocite{latha2024revolutionizing}. Such public repositories have enabled various CNN-based breast cancer detection and classification studies.

\section{History}
\label{sec:history}
Computer-aided detection (CAD) systems for breast cancer originated in the 1980s with the advent of digital mammography, marking the beginning of the use of computer systems to assist radiologists. By the mid-1980s, major strides were made in the development of algorithms for use in mammography (and also chest radiographs) \autocite{https://doi.org/10.1118/1.3013555}. These early CAD systems used hand-engineered image processing, and statistical classifiers to detect calcifications or masses. In the 1990s, the first studies that involved neural networks for CAD appeared. These involved the application of the first convolutional neural network to the detection of mammographic microcalcification \autocite{https://doi.org/10.1118/1.3013555}. In subsequent years, the use of more traditional machine learning (ML) methods such as support vector machines (SVMs), and decision trees was also applied to breast pathology, and imaging, using handcrafted features like texture, shape, and histogram statistics to classify regions as benign or malignant. Today, CNNs are dominant in both mammography, and histopathology tasks, without using classical pipelines that extract features for a separate classifier. The use of CNNs allows for direct learning of features from pixels, allowing for better accuracy, and generalisation \autocite{araujo2017classification}. That is, studies have shown that CNN-extracted features used in an SVM achieved a sensitivity of 95.6\% on histology slides \autocite{araujo2017classification}. In general, CNN-based models outperform traditional feature-based SVMs or random forest classifiers in many breast image data sets, although SVMs, and trees are still used for certain tasks, such as radiomics-based subtype classification \autocite{guo2024machine}. The disadvantages of using CNNs usually include the necessity of large, labelled data sets, and heavy computational resources for their training, and inference. This, however, comes at the advantage of automated feature learning, and the identification of subtle, complex patterns that classical machine learning models struggle to come to terms with. On the other hand, SVMs and decision trees can work with smaller data sets, but depend on the design of domain-expert features, and may not generalise as well to unseen data \autocite{araujo2017classification}.

\section{Convolutional Neural Network Architectures, and Methods}
\label{sec:cnn-arch}
Modern CNN-based architectures for breast cancer detection are typically based on established architectures (such as ResNet, Inception, VGG, DenseNet, and EfficientNet) designed for medical imaging. Transfer learning is a common approach, where pre-trained models are fine-tuned on medical scans to compensate for limited annotated data \autocite{srikantamurthy2023classification}. This approach has been shown to outperform using CNNs as fixed feature extractors. Data augmentation such as flips, rotations, colour jittering, and GAN-based augmentation is also commonly used, particularly in ultrasound, and pathology where data sets are limited \autocite{latha2024revolutionizing}. Namely, Gupta et al. state that the random flipping, and rotation of ultrasound images, help to overcome class imbalance on the BUSI data set \autocite{latha2024revolutionizing}. Explainability methods such as Grad-CAM heat maps are incorporated to visualise salient regions, and build trust in the model \autocite{latha2024revolutionizing} \autocite{kaba2024explainable}. In segmentation tasks (such as lesions), encoder-decoder networks such as U-Net remain the industry standard, producing critical performance on medical image segmentation tasks \autocite{jiang2024deep}.

\section{Mammography-based detection}
\label{sec:mammography-cnn}
CNNs have demonstrated high diagnostic accuracy by classifying whole images or localised patches of mammograms. Studies often use the DDSM or INbreast data set, and the Digital Breast Tomosynthesis (DBT) data set. That is, a commercial deep CNN-based system, trained on about 1,000 mammograms, reached an AUC score of 0.82, comparable to an expert radiologist \autocite{carriero2024deep}. Retrospective evaluations indicate that AI-powered computer-aided diagnosis (CAD) systems can improve the sensitivity of the radiologist, so that their cancer detection rate increased from 51\% to 62\% after the introduction of AI aid as evidenced by Watanabe et al. \autocite{carriero2024deep}. In a similar study, Kim et al. reported that an unassisted AI-CAD system achieved an AUC of 0.94, outperforming the radiologist's AUC of 0.88 \autocite{carriero2024deep}. Akselrod-Ballin et al. achieved an AUC of 0.91 using a large linked data set of mammograms, and patient records \autocite{carriero2024deep}. In large screening challenges (with around 85,000 training images, and 68,000 external images), the top CNN ensembles reached an AUC of 0.90 on held out mammograms \autocite{carriero2024deep}. Although still below expert performance, the combination of radiologists with AI yielded an AUC of 0.94, indicating the performance gains experienced when the two are synergised. CNNs have also been applied to lesion detection, and segmentation, with a particular case in which a U-Net-like model, trained on a CBIS-DDSM data set, detected masses with 95.7\% sensitivity, and a Dice score of 74.5\% \autocite{jiang2024deep}.

Popular CNN models for mammography include ResNet50, VGG16/19, Inception, and EfficientNet. For tasks such as the detection of mass classification or microcalcification detection, such networks often outperform more traditional feature-based methods \autocite{wang2024mammography} \autocite{carriero2024deep}. That is, Shen et al. found that a fine-tuned CNN achieved 88\% accuracy while classifying mammographic tumours, outperforming radiologists who achieved 83\% accuracy \autocite{wang2024mammography}. In another study, Yala et al. report CNN risk models with an AUC of 0.84 versus an AUC of 0.77 for radiologists \autocite{wang2024mammography}. Object detection models such as Faster-RCNN, and RetinaNet were also used in the Agarwal et al. study, which achieved 99\% accuracy for malignant-mass detection in the INbreast data set, when Faster-RCNN was applied to full-field mammograms \autocite{jiang2024deep}. More recent work uses transformer-based models using a pretrained vision transformer, and segmentation, which reaches a 99.96\% accuracy in INbreast, by first extracting a mass ROI \autocite{kumar2024segmentation}. In total, CNNs on mammograms achieve a high AUC value, often similar to or exceeding the performance of an average radiologist \autocite{carriero2024deep}.

\section{Histopathology-based detection}
\label{sec:histopathology-cnn}
CNNs have also been applied to histopathological slides for cancer classification. The BreakHis data set (containing microscopic tissue patches at 40x-400x magnification) has been the de facto benchmark for this. Basic CNNs like ResNet, and VGG achieve more than 90\% precision in binary classification tasks on the BreakHis data set (when classifying benign from malignant cancers) \autocite{jiang2024deep}. In Han et al. a CNN was applied to the BreakHis data set, achieving a precision of 93.2\% \autocite{jiang2024deep}. More complex architectures can significantly improve performance, such as in Munikoti et al., which combines a CNN with an LSTM (using ImageNet transfer learning), and achieves a 99\% precision in binary classification in the BreakHis data set \autocite{srikantamurthy2023classification}. Such deep learning models significantly outperform more traditional machine learning methods like Rao et al., which found that CNNs (with transfer learning) superseded SVMs or random forests in histological data \autocite{yusoff2023accuracy}. In addition to patch classification, CNNs like U-net are used to segment nuclei or tumour regions to aid analysis \autocite{jiang2024deep}. Modern approaches use attention or multiscale to capture complex tissue contexts \autocite{srikantamurthy2023classification}. In conclusion, CNNs on histopathology often achieve very high accuracies of 90-99\% on curated data sets \autocite{srikantamurthy2023classification} \autocite{yusoff2023accuracy}, taking advantage of deep architectures, and large-scale patch augmentation.

\section{Ultrasound-based detection}
\label{sec:ultrasound-cnn}
When breast tissue is dense, ultrasound imaging is often used as an imaging modality. The use of this poses issues relating to speckle noise, and operator variability. CNNs have shown remarkable results despite this. Since public ultrasound data sets are smaller, transfer learning from networks such as VGG-16, VGG-19, and AlexNet is often used. Wang et al. applied a multiview CNN with ImageNet pre-training to a breast ultrasound, and achieved an AUC of 0.9468 \autocite{wang2024mammography}. In another study, Gupta et al. fine-tuned EfficientNet-B7 on the BUSI data set, and reported a 99.1\% precision \autocite{latha2024revolutionizing}. Augmentation methods such as rotations, and colour jitter were used, and Grad-CAM explanation was used to focus on lesion features \autocite{latha2024revolutionizing}. In addition, Rahman et al. deduced that a fine-tuned InceptionV3 model outperformed VGG-19 for ultrasound tumour classification \autocite{ayana2021transfer}. In general, ultrasound CNNs commonly exceed 90\% accuracy, with transfer learning, and data augmentation necessary to compensate for limited sample sizes.

\section{Transfer Learning, Data Augmentation, and Explainability}
\label{sec:transfer-learning}
Since annotated breast imaging data is limited, transfer learning, and augmentation are essential. Almost all recent studies initialise CNNs with ImageNet weights, and fine-tune on medical images \autocite{srikantamurthy2023classification}. Fine-tuning routinely outperforms training from scratch or using CNNs as pure feature extractors. Strategies for augmentation, including geometric transforms, colour perturbations, and GAN-based synthesis, help to ease overfitting problems. That is, EfficientNet-based ultrasound classifiers used random flipping, rotation, and colour jitter to improve minority malignant cases \autocite{latha2024revolutionizing}. Other histological studies have also used flips, rotations, and staining augmentation to improve the diversity of tissue patches \autocite{srikantamurthy2023classification}.

Explainable AI is becoming increasingly important in medical imaging, where methods such as heat maps (such as Grad-CAM, and saliency maps) are applied to highlight image regions to make predictions \autocite{latha2024revolutionizing} \autocite{kaba2024explainable}. This helps verify that CNNs focus on tumours rather than artefacts. Studies report overlaying class activation maps on mammographs or ultrasound to show ROI localisation. That is, EfficientNet-B7 visualised Grad-CAM heat maps on ultrasound to confirm the focus of the injury \autocite{latha2024revolutionizing}. Explainability is also explored through feature attribution, and partial dependence, but Grad-CAM remains popular \autocite{latha2024revolutionizing} \autocite{kaba2024explainable}.

\section{Performance vs. Traditional Methods, and Radiologists}
\label{sec:performance}
CNN-based systems have been shown to outperform classical machine learning algorithms, and approximate expert-level analysis. Deep models have reduced false positives relative to older CAD systems, and improved sensitivity. In mammography tasks, CNNs matched or superseded radiologist sensitivity as demonstrated by Becker et al., which reported an equal AUC of 0.82 for a commercial CNN, and human experts \autocite{carriero2024deep}. In larger trials, deep learning models achieved an AUC of 0.86 to 0.94, while experts scored slightly better, but, when combining AI with human experts, recall rates were consistently reduced \autocite{carriero2024deep}. In ultrasound, CNNs have outperformed conventional classifiers, such as SVMs, and decision trees, when given enough data \autocite{yusoff2023accuracy}. In histopathology, CNNs significantly outperform custom feature methods, where hybrid CNN and TL models reached an accuracy of 97\% in BreakHis, and classical machine learning models rarely exceeded 90\% on the same task \autocite{yusoff2023accuracy}. Thus, studies indicate that CNNs offer excellent sensitivity, and specificity across different modalities \autocite{carriero2024deep} \autocite{wang2024mammography}, demonstrating their practical utility within CAD systems.

\end{document}
