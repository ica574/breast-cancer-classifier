\documentclass[../main]{subfiles}

\begin{document}
\chapter{Implementation}
This chapter outlines the technical implementation of the breast cancer
detection system, with a focus on data handling, model architecture, training
methodology, and explainability. The system was implemented using TensorFlow,
and Keras, running atop the Python interpreter. The data ingestion, and
preprocessing steps will be covered alongside the architecture, and training of
the convolutional neural networks used. Strategies to improve generalisation
will be detailed, as well as the explainability module, and capturing, and
visualising the results. Snippets of code are shown to demonstrate key points,
with the full listings available in the appendix.

\section{Setup}
The necessary libraries are first imported, and the necessary helper functions
are defined at the beginning of the notebook, to be used throughout. Namely, a
garbage collection function is defined to clear as much memory as possible to
prevent the system from crashing. This is called throughout the notebook. The
function is called first to set the stage for the memory to be occupied solely
by the subroutines in the following stages of the implementation.

\section{Data Manipulation}
This section focuses on loading, and preparing data for training, and testing
from TFRecord files. A random seed is firstly set so that results are
reproducibile. Due to the limitations of the hardware utilised for this
project, a small subset of the DDSM dataset, sourced from Kaggle, was utilised,
with pre-augmented images. The dataset was provided in the form of TensorFlow's
TFRecord format, which is a binary format that allows for efficient storage and
retrieval of large datasets. Each TFRecord file contains three fields,
\texttt{image}, \texttt{label}, and \texttt{label\_normal}. The \texttt{image}
field contains the raw, flattened image data, the \texttt{label} field contains
the label for the image in the form of a binary integer class (0 for benign,
and 1 for malignant classifications), and the \texttt{label\_normal} field
contains a normalised version of the label for direct use in training.


Two empty lists are first initialised for images, and labels with a dictionary
defined that specifies the structure of TFRecord data. A function is then
defined to process the TFRecord files, and store the results into the
previously defined \texttt{images}, and \texttt{labels} lists. 

\begin{lstlisting}[language=Python, caption={Data ingestion from TFRecord files.}]
def read_file(files):
    mydata = (
        tf.data.TFRecordDataset(files, num_parallel_reads=4)
        .shuffle(buffer_size=10000)
        .cache()
    )
    mydata = mydata.map(
        lambda x: tf.io.parse_example(x, features), num_parallel_calls=4
    )
    for image_features in mydata:
        image = tf.io.decode_raw(image_features["image"], tf.uint8)
        image = tf.reshape(image, [299, 299])
        image = np.asarray(image)
        images.append(image)
        labels.append(image_features["label"].numpy())
\end{lstlisting}

The garbage collection function is invoked before the files are parsed, to
avoid any memory issues. The target TFRecord files are defined in a list, prior
to being processed. The parsing function is finally called on the previously
defined list.

\subsection{Preparing Data for Training}
This section will discuss the cells that process the already loaded training
data to make it more understandable to the CNN later in the notebook. Each
image is first saved as a \texttt{.jpg} file in the \texttt{train} directory
with enumerated filenames. The list of labels is saved as a CSV file within the
same directory. The labels are then loaded, and converted to boolean format,
ideal for the binary classification task the CNN is to perform. The list of
\texttt{.jpg} files from the \texttt{train} directory is retrieving using
\texttt{glob}, with the numeric part of the filename extracted, and storing in
the \texttt{mysplit} list for further sorting. A Pandas DataFrame is defined
for training, with columns to store the image paths, and binary label, all
sorted by their numeric value.

\subsection{Preparing Data for Testing}
This section details methods similar to those seen within the previous section,
but more suited towards preparing testing data. The following cell begins by
loading test images, and labels from NumPy files, and concatenates them to form
\texttt{x\_test} for images, and \texttt{y\_test} for labels. The
\texttt{y\_test} array is then converted to boolean format as was done for
training labels. The testing data is then saved as \texttt{.jpg} files within
the \texttt{test} directory with enumerated filenames. The test labels are then
loaded from the CSV file, and casted as strings. Similar to the previous
section, the filenames are retrieved as a list of \texttt{.jpg} files using
\texttt{glob}, with the numeric part of the filename extracted via
\texttt{glob}. A Pandas DataFrame is finally created with this information.

\subsection{Visualisation}
The dataset is visualised within this section, with the garbage
collection function being called first to alleviate the system's memory
as much as possible.
   
\subsection{Data Generators}
The data processed in the previous sections is finally encapsulated into
generators to feed it into the model during training, and testing. An
\texttt{ImageDataGenerator} object is first defined to preprocess images by
rescaling their pixel values. Each pixel is normalised into the range 0, to
255 by being divided by 255. This is known to improve the model's training
efficiency, and performance. The training generator yields batches of
preprocessed training images, and their corresponding labels from the
\texttt{traindf} DataFrame. This feeds the model during training. A similar
generator is made for the testing set. As for the ViT model, the training, and
testing data is further encapsulated into a HuggingFace dataset.

\subsection{Data Augmentation}
The dataset is already pre-augmented, but further augmentation is applied to
the training data for the ViT model to improve its generalisation capabilities.
This is done by applying random transformations to the training images, such as
horizontal flipping, and rotations. The augmentation is done using the
\texttt{transforms} function from the \texttt{torchvision} library, which
allows for easy application of various transformations to the images. The
augmentation is defined as follows, and is applied to the training, and testing
data before it is fed into the model.

\begin{lstlisting}[language=Python, caption={Data augmentation for the ViT model.}]
augment = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
])

def transform(example_batch):
    images = [augment(Image.open(f).convert("RGB")) for f in example_batch["file"]]
    inputs = processor(images, return_tensors="pt")
    inputs["labels"] = [int(lbl) for lbl in example_batch["label"]]
    return inputs

prepared_train_ds = train_dataset.with_transform(transform)
prepared_test_ds = test_dataset.with_transform(transform)
\end{lstlisting}

\subsection{Class Imbalance}
The dataset utilised appears to be imbalanced with
an 87:13 ratio of negative to positive samples. Without intervention, this leads
to the model's outputs being skewed in a detrimental manner, where more false
samples are likely to be produced. In order to address this, a callback is
utilised from the standard TensorFlow API to compute class weights, and apply
them during training. This scales the classes according to each one's scarcity
in an attempt to offset the class imbalance, and correct the behaviour of the
model. The corrections are computed by the following snippet.

\begin{lstlisting}[language=Python, caption={Correcting the class imbalance.}]
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weights = dict(enumerate(class_weights))
\end{lstlisting}

\noindent The resultant weights are then passed into the model during training,
as an argument to the model's \texttt{fit} method.

\section{Model Definition}
Four models are defined, three of which are CNN-based, and another one which is
transformer-based. Each CNN is defined with the same classification head, but
an altered base. These models are made to handle binary classification tasks,
which are to be trained on breast cancer tissue mammograms. The first model is
an entirely custom CNN, which is based upon a series of \texttt{Conv2D},
\texttt{BatchNormalization}, and \texttt{MaxPooling2D} layers. For the
classifier head, these are then followed by \texttt{Dense}, and
\texttt{GlobalAveragePooling2D} layers, and fine-tuned accordingly.

\begin{lstlisting}[language=Python, caption={Custom CNN definition.}]
input_tensor = Input(shape=(299, 299, 3))

x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_tensor)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2))(x)

x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2))(x)

x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2))(x)

x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2))(x)

x = GlobalAveragePooling2D()(x)
output = Dense(1, activation='sigmoid', name='classifier')(x)
\end{lstlisting}

\noindent The second model introduces transfer learning by supplanting the basis
of the model, excluding the classifier head, with a VGG16 model. This
significantly simplifies the model definition, and yields benefits in
performance when classifying samples.

\begin{lstlisting}[language=Python, caption={VGG16 model definition.}]
vgg16_base = VGG16(weights="imagenet", include_top=False, input_shape=(299, 299, 3))
classifier_head = vgg16_base.output
classifier_head = GlobalAveragePooling2D()(classifier_head)
classifier_head = Dense(1, activation="sigmoid", name="classifier")(classifier_head)
vgg16_model = Model(vgg16_base.input, classifier_head)
\end{lstlisting}

\noindent Something similar is done with the third model, where ResNet50 is used instead of VGG16.

\begin{lstlisting}[language=Python, caption={ResNet50 model definition.}]
resnet50_base = ResNet50V2(weights="imagenet", include_top=False, input_shape=(299, 299, 3))
classifier_head = resnet50_base.output
classifier_head = GlobalAveragePooling2D()(classifier_head)
classifier_head = Dense(1, activation="sigmoid", name="classifier")(classifier_head)
resnet50_model = Model(resnet50_base.input, classifier_head)
resnet50_model.summary()
\end{lstlisting}

\noindent The fourth model is transformer-based, and uses the \texttt{ViT} or
Vision Transformer architecture. The foundation model used is the
\texttt{facebook/deit-tiny-patch16-224} model provided by Facebook, which is a
small \texttt{ViT} model, and is more suited for the hardware used in this
project. The transformer is fine-tuned on the breast cancer dataset. It is also
worth noting that this model utilises a PyTorch backend, and is loaded using
HuggingFace's \texttt{transformers} library. Due to this, the programmatic
definitions for training will slightly differ from the other CNN-based models.

\begin{lstlisting}[language=Python, caption={Vision Transformer model definition.}]
labels_list = ["Negative", "Positive"]
model = ViTForImageClassification.from_pretrained(
    'facebook/deit-tiny-patch16-224',
    num_labels=len(labels_list),
    id2label={str(i): c for i, c in enumerate(labels_list)},
    label2id={c: str(i) for i, c in enumerate(labels_list)},
    ignore_mismatched_sizes=True
)
\end{lstlisting}


\noindent The idea behind defining these models is to compare, and contrast
between the different bases, and artchitectures for performance gains from the
use of transfer learning, versus using an entirely custom model.

\subsection{Training the Model}
The model will now be trained, based on the previous definition. Garbage
collection is called prior to training, to avoid memory issues. Important
callbacks are first defined to stop training if parameters like validation loss
do not improve, and for the model with best validation accuracy to be saved. The
model is compiled with an \texttt{adagrad} optimisation function, and binary
cross-entropy loss function. The model is finally trained, with data being fed
from the previously defined generators.

\begin{lstlisting}[language=Python, caption={Compiling the custom CNN using the Adam optimiser, and a binary cross-entropy loss function.}]
custom_cnn.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss='binary_crossentropy',
    metrics=['accuracy']
)
\end{lstlisting}

\noindent The following snippet shows the point at which the model is trained,
including epoch count, inclusion of callbacks, validation data, and class
weights.

\begin{lstlisting}[language=Python, caption={Model training of the custom CNN.}]
custom_cnn_history = custom_cnn.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=5,
    callbacks=[checkpoint, early],
    validation_data=test_generator,
    class_weight=class_weights
)
\end{lstlisting}

\subsection{Evaluating the Model}
This section evaluates the previously trained models using standard metrics like
accuracy, and loss on the training, and validation sets. These metrics are
extracted into variables for later use from the performance of the model across
different epochs during training. These are then plotted, to visualise how the
model's accuracy, and loss scores improved. Predictions are made on the test set
in order to generate a final classification report, and confusion matrix. The
previously-made predictions, and actual values are gathered, and fed into
\texttt{scikit-learn} to generate the confusion matrix. The results are then
plotted using \texttt{seaborn}. The same values are used to generate the final
classification report. The performances of the different models are plotted on
the same accuracy, and loss graphs in order to demonstrate a clear distinction
of performance between the different models.
 
\section{Model Explanation}
This final section touches upon XAI or explainable AI, by implementing
\emph{Grad-CAM} through the \texttt{tf-explain} module. This provides a visual
indication of the parts of the image that influenced the classification made of
the model, explaining the results achieved. This is done for the first ten
images within the testing set, but can easily be done for any other image as
well. The following function accepts a path to an image, the model used, and
it's final layer, and returns the explained version of the image, and the
classification result. A series of images is then passed through the
aforementioned function, and plotted into a singular figure. In the above plot,
warmer colours indicate higher influence on the model's predicted class, and
cooler colours the opposite.

\end{document}
