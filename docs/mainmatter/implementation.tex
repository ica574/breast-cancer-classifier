\documentclass[../main]{subfiles}

\begin{document}
\chapter{Implementation}
This chapter outlines the technical implementation of the breast cancer
detection system, with a focus on data handling, model architecture, training
methodology, and explainability. The system was implemented using TensorFlow,
and Keras, running atop the Python interpreter. The data ingestion, and
preprocessing steps will be covered alongside the architecture, and training of
the convolutional neural networks used. Strategies to improve generalisation
will be detailed, as well as the explainability module, and capturing, and
visualising the results. Snippets of code are shown to demonstrate key points,
with the full listings available in the appendix.

\section{Setup}
The necessary libraries are first imported, and the necessary helper functions
are defined at the beginning of the notebook, to be used throughout. Namely, a
garbage collection function is defined to clear as much memory as possible to
prevent the system from crashing. This is called throughout the notebook. The
function is called first to set the stage for the memory to be occupied solely
by the subroutines in the following stages of the implementation.

\section{Data Manipulation}
This section focuses on loading, and preparing data for training, and testing
from TFRecord files. A random seed is firstly set so that results are
reproducibile. Due to the limitations of the hardware utilised for this
project, a small subset of the DDSM dataset, sourced from Kaggle, was utilised,
with pre-augmented images. The dataset was provided in the form of TensorFlow's
TFRecord format, which is a binary format that allows for efficient storage and
retrieval of large datasets. Each TFRecord file contains three fields,
\texttt{image}, \texttt{label}, and \texttt{label\_normal}. The \texttt{image}
field contains the raw, flattened image data, the \texttt{label} field contains
the label for the image in the form of a binary integer class (0 for benign,
and 1 for malignant classifications), and the \texttt{label\_normal} field
contains a normalised version of the label for direct use in training.


Two empty lists are first initialised for images, and labels with a dictionary
defined that specifies the structure of TFRecord data. A function is then
defined to process the TFRecord files, and store the results into the
previously defined \texttt{images}, and \texttt{labels} lists. 

\begin{lstlisting}[language=Python, caption=Python example]
def read_file(files):
    mydata = (
        tf.data.TFRecordDataset(files, num_parallel_reads=4)
        .shuffle(buffer_size=10000)
        .cache()
    )
    mydata = mydata.map(
        lambda x: tf.io.parse_example(x, features), num_parallel_calls=4
    )
    for image_features in mydata:
        image = tf.io.decode_raw(image_features["image"], tf.uint8)
        image = tf.reshape(image, [299, 299])
        image = np.asarray(image)
        images.append(image)
        labels.append(image_features["label"].numpy())
\end{lstlisting}

The garbage collection function is invoked before the files are parsed, to
avoid any memory issues. The target TFRecord files are defined in a list, prior
to being processed. The parsing function is finally called on the previously
defined list.

\subsection{Preparing Data for Training}
This section will discuss the cells that process the already loaded training
data to make it more understandable to the CNN later in the notebook. Each
image is first saved as a \texttt{.jpg} file in the \texttt{train} directory
with enumerated filenames. The list of labels is saved as a CSV file within the
same directory. The labels are then loaded, and converted to boolean format,
ideal for the binary classification task the CNN is to perform. The list of
\texttt{.jpg} files from the \texttt{train} directory is retrieving using
\texttt{glob}, with the numeric part of the filename extracted, and storing in
the \texttt{mysplit} list for further sorting. A Pandas DataFrame is defined
for training, with columns to store the image paths, and binary label, all
sorted by their numeric value.

\subsection{Preparing Data for Testing}
This section details methods similar to those seen within the previous section,
but more suited towards preparing testing data. The following cell begins by
loading test images, and labels from NumPy files, and concatenates them to form
\texttt{x\_test} for images, and \texttt{y\_test} for labels. The
\texttt{y\_test} array is then converted to boolean format as was done for
training labels. The testing data is then saved as \texttt{.jpg} files within
the \texttt{test} directory with enumerated filenames. The test labels are then
loaded from the CSV file, and casted as strings. Similar to the previous
section, the filenames are retrieved as a list of \texttt{.jpg} files using
\texttt{glob}, with the numeric part of the filename extracted via
\texttt{glob}. A Pandas DataFrame is finally created with this information.

\subsection{Visualisation}
The dataset is visualised within this section, with the garbage
collection function being called first to alleviate the system's memory
as much as possible.
   
\subsection{Data Generators}
The data processed in the previous sections is finally encapsulated into
generators to feed it into the model during training, and testing. An
\texttt{ImageDataGenerator} object is first defined to preprocess images by
rescaling their pixel values. Each pixel is normalised into the range 0, to
255 by being divided by 255. This is known to improve the model's training
efficiency, and performance. The training generator yields batches of
preprocessed training images, and their corresponding labels from the
\texttt{traindf} DataFrame. This feeds the model during training. A similar
generator is made for the testing set.

\section{Model Definition}
The model used for binary classification of breast tissue is a
convolutional neural network, based upon a frozen \texttt{VGG16} layer,
that uses pre-trained \texttt{ImageNet} weights. This is then followed
by \texttt{Dense}, and \texttt{GlobalAveragePooling2D} layers, and
fine-tuned accordingly.

\begin{lstlisting}[language=Python, caption=Python example]
myconv = VGG16(weights="imagenet", include_top=False, input_shape=(299, 299, 3))
x = myconv.output
x = GlobalAveragePooling2D()(x)
x = Dense(1, activation="sigmoid", name="classifier")(x)
model = Model(myconv.input, x)
\end{lstlisting}

\subsection{Training the Model}
The model will now be trained, based on the previous definition. Garbage
collection is called prior to training, to avoid memory issues. Important
callbacks are first defined to stop training if parameters like validation loss
do not improve, and for the model with best validation accuracy to be saved. The
model is compiled with an \texttt{adagrad} optimisation function, and binary
cross-entropy loss function. The model is finally trained, with data being fed
from the previously defined generators.

\subsection{Evaluating the Model}
This section evaluates the previously trained model using standard metrics like
accuracy, and loss on the training, and validation sets. In the following cell,
these metrics are extracted into variables for later use from the performance of
the model across different epochs during training. This is then plotted, to
visualise how the model's accuracy, and loss scores improved. Predictions are
made on the test set are made in order to generate a final classification
report, and confusion matrix. The previously-made predictions, and actual values
are gathered, and fed into \texttt{scikit-learn} to generate the confusion
matrix. The results are then plotted using \texttt{seaborn}. The same values are
used to generate the final classification report.
 
\section{Model Explanation}
This final section touches upon XAI or explainable AI, by implementing
\emph{Grad-CAM} through the \texttt{tf-explain} module. This provides a visual
indication of the parts of the image that influenced the classification made of
the model, explaining the results achieved. This is done for the first ten
images within the testing set, but can easily be done for any other image as
well. The following function accepts a path to an image, the model used, and
it's final layer, and returns the explained version of the image, and the
classification result. A series of images is then passed through the
aforementioned function, and plotted into a singular figure. In the above plot,
warmer colours indicate higher influence on the model's predicted class, and
cooler colours the opposite.

\end{document}
