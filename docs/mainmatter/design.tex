\documentclass[../main]{subfiles}

\begin{document}
\chapter{Design}
Breast cancer is a leading cause of death amongst women worldwide, and early detection is paramount to improve the chances of survival. Computer-aided detection (CAD) systems have long been used to assist in screening, and diagnosis workflows for long, but recent developments in convolutional neural networks (CNNs) have significantly boosted the rates of detection on natural image benchmarks. The aim of this project is to leverage state-of-the-art CNN architectures to build a decision support system for breast cancer detection in mammograms. The Digital Database for Screening Mammography (DDSM) will be used to train, and evaluate a CNN model, and the results will be used to indicate the presence of malignant tumours. Thus, the primary objective is to develop, and evaluate a CNN, that matches or exceeds the single-reader accuracy commonly achieved by radiologists in large-scale screening programmes.

\section{Domain Context, and User Analysis}
Mammography screening programs capture images of the breast, and use them to detect early signs of breast cancer. Radiologists are responsible for viewing multiple images per week. In this context, false negatives lead to delayed diagnoses that may pose serious health consequences, whilst false positives impose unnecessary stress on patients, and the system itself, with procedures like biopsies, and follow-up imaging creating an operational burden. Therefore, any AI-powered decision support system must prioritise very high sensitivity while maintaining a reasonable specificity to avoid recalls.

The primary users of this system will be professional radiologists, who will utilise suggestions from it, and clinicians who monitor screening quality, and throughput values. Radiologists will require not only malignancy scores, but also explainable outputs demonstrating where the model has detected abnormalities for validation purposes, and to manage trust issues in the system. Many published works lack sufficient end-to-end system design, and evaluation. By concentrating on both model performance, and system usability, this project fills a gap in showing a deployable prototype complete with explainable outputs, and inference benchmarks.

\section{System Architecture, and Design}
The resultant system will be comprised of the following components: data preprocessing, model training, and evaluation, and explainability.

\begin{itemize}
	\item \textbf{Data Preprocessing} \textemdash\ The DDSM dataset will be appropriately preprocessed to ensure that the images are in a suitable format for the model. This will include resizing, normalisation, and augmentation of the images to improve the model's performance. Thus, horizontal flipping, rotations, and contrast adjustments may be applied on the fly to increase the diversity of the training data, and reduce overfitting. The dataset will also be split into training, validation, and test sets to ensure equal class balance across malignant, and benign classes.
	\item \textbf{Model Training, and Evaluation} \textemdash\ Multiple CNN models will be trained on the DDSM dataset, being constructed with TensorFlow, and Keras. The model will then be evaluated using metrics like accuracy, precision, recall, and F1-score.
	\item \textbf{Explainability} \textemdash\ Grad-CAM will be used to overlay heatmaps on input mammograms, demonstrating various regions that drive the model's prediction.
\end{itemize}

\section{Technologies, and Methods}
In light of the previous components, mature, and well-supported libraries were selected to ensure that the system is easy to maintain, and extend.

\begin{itemize}
	\item \textbf{Data Handling} \textemdash\ Python as the fundamental language of choice for the entire stack, NumPy, and Pandas for data manipulation tasks, and OpenCV for image processing tasks.
	\item \textbf{Deep Learning} \textemdash\ TensorFlow for neural networks, with Keras frontend for ease of use, and fast prototyping.
	\item \textbf{Hyperparameter Tuning} \textemdash\ Optuna for automated, and efficient hyperparameter tuning, and model selection.
	\item \textbf{Explainability} \textemdash\ Grad-CAM through tf-keras-vis for visualising the model's attention on the input image, and highlighting the areas of interest.
\end{itemize}

\section{Model}
The core of the system is a deep feature extractor powered by a convolutional neural network (CNN). Multiple methods will be explored, including alternative architectures like vision transformers (ViTs), but initially, a custom CNN will be trialled. Alternative bases like VGG16, and ResNet50 will also be evaluated. This provides proven performance on breast image tasks, and manageable complexity \autocite{fatima2025application}. VGG16's uniform 16-layer architecture adapts well via transfer learning from ImageNet, with prior work achieving a high accuracy on the BreakHis histopathology dataset by fine-tuning VGG16 \autocite{fatima2025application}. Additionally, VGG16 has shown excellent generalisation on unbalanced data \autocite{fatima2025application}, making it suitable for the DDSM dataset's class imbalance.

\section{Clinical Integration}
The system could be integrated into existing radiology workflows, providing a decision support tool that assists radiologists in identifying potential malignancies. The model's outputs, including the malignancy score and Grad-CAM heatmaps, may be presented in the form of a user interface that is accessed by clinicians, and radiologists. Additionally, the system could log the model's predictions, and the corresponding images, to create a database of cases that can be used for further research, and development. This would be facilitated by the use of a two-tier architecture web application, with a RESTful API for the model, and a frontend for user interaction. Requests to the API will return the model's predictions, and Grad-CAM heatmaps, which can then be displayed in the frontend. As for the logging requests, a relational database such as PostgreSQL could be used to store the model's predictions, and the corresponding images, along with metadata such as the date, time, and user ID. This would allow for easy retrieval of cases for further analysis, and research. To set up the APIs, a web framework like FastAPI could be used, which is designed for building APIs quickly, and efficiently. FastAPI is built on top of Starlette, and Pydantic, and provides automatic generation of OpenAPI documentation, which can be used to document the API endpoints, and their parameters. The frontend could be built using a modern JavaScript framework like Next.js, which would allow for a responsive, and interactive user interface. The frontend would communicate with the backend API to retrieve the model's predictions, and Grad-CAM heatmaps, which can then be displayed to the user. Support for asynchronous requests, and real-time updates could be implemented which would allow for a more responsive user experience, and makes the system able to scale. The frontend could also include features such as user authentication, and authorisation, to ensure that only authorised users can access the system, and view logs. As for GPU acceleration, the model could be deployed on a cloud platform such as Google Cloud Platform (GCP), or Amazon Web Services (AWS), which provides CUDA-comptatible GPU instances that can be used to accelerate model inference. This would allow for faster processing of images, and reduce the time taken to generate predictions. Additionally, the cloud platform could be used to store the model's weights, and configuration files, which can be easily accessed by the frontend, and backend APIs. In this pursuit, a series of MLOps pipelines could be developed in order to facilitate the training, and deployment of models, and reduce the time taken in between prototyping, and production releases. Finally, horizontal scaling could be deployed through Kubernetes, which would allow for the system to handle a large number of requests, and scale up or down based on demand. However, the latter would mostly be important in cases with very high throughput that would likely not be seen within a single, or even a few hospitals. This would probably be of significance should the system be deployed as a form of SaaS, where multiple hospitals around the globe make use of the service simultaneously. In this case, the system may also need to use message queues, such as RabbitMQ, or Kafka, and be split into microservices to handle the high volume of requests, and ensure that the system remains responsive.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/architectural_diagram.png}
    \caption{Mock System Architecture}
    \label{fig:system_architecture}
\end{figure}

\noindent The above is a mock system architecture that incorporates some of the elements described previously. It offers a simplified version of the system that would provide an adequate starting point for the project, and can be extended as needed. CI/CD pipelines are even included through the use of Jenkins, and GitHub where the \texttt{jenkinsfile} is hosted within the project's repoistories. Blue-green load balancing is also introduced to manage API versions within requests, and JWT for authentication. Amazon Aurora is used as a database, which is PostgreSQL compatible.

\section{Plan of Evaluation}
The project as a whole will be evaluated using a variety of metrics. For the model itself, the most critical part of the system, the standard evaluation metrics of accuracy, precision, recall, and F1-score will be used to assess its performance on the DDSM dataset, including the Area Under the Receiver Operating Characteristic Curve (AUC). Together with Grad-CAM heatmaps, which will show the model's attention on the input images, these metrics will provide a comprehensive outlook on the model's classification performance. The evaluation will focus on the model's performance, and the explainability of the outputs.

\end{document}
