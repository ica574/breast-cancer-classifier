\documentclass[../main]{subfiles}

\begin{document}
\chapter{Design}
Breast cancer is a leading cause of death among women worldwide, and early detection is paramount to improving the chances of survival. Computer-aided detection (CAD) systems have long been used to assist in screening, and diagnosis workflows for a long time, but recent developments in convolutional neural networks (CNNs) have significantly boosted the rates of detection on natural image benchmarks. This project aims to leverage state-of-the-art CNN architectures to build a decision support system for breast cancer detection on mammograms. The Digital Database for Screening Mammography (DDSM) will be used to train, and evaluate a CNN model, and the results will be used to indicate the presence of malignant tumours. Thus, the primary objective is to develop, and evaluate a CNN that matches or exceeds the single-reader accuracy commonly achieved by radiologists in large-scale screening programs.

\section{Domain Context, and User Analysis}
Mammography screening programs capture images of the breast, and use them to detect early signs of breast cancer. Radiologists are responsible for viewing multiple images per week. In this context, false negatives lead to delayed diagnoses that may pose serious health consequences, whilst false positives impose unnecessary stress on patients, and the system itself, with procedures like biopsies, and follow-up imaging creating an operational burden. Therefore, any AI-powered decision support system must prioritise very high sensitivity while maintaining a reasonable specificity to avoid recalls.

The primary users of this system will be professional radiologists who will use its suggestions, and clinicians who monitor screening quality, and throughput values. Radiologists will require not only malignancy scores, but also explainable outputs demonstrating where the model has detected abnormalities for validation purposes, and to manage trust issues in the system. Many published works lack sufficient end-to-end system design, and evaluation. By concentrating on both model performance, and system usability, this project fills a gap in showing a deployable prototype complete with explainable outputs, and inference benchmarks.

\section{System Architecture, and Design}
The resultant system will consist of the following components: data pre-processing, model training, and evaluation, and explainability.

\begin{itemize}
	\item \textbf{Data Preprocessing} \textemdash\ The DDSM data set will be appropriately preprocessed to ensure that the images are in a suitable format for the model. This will include resizing, normalisation, and augmentation of the images to improve the model's performance. Thus, horizontal flipping, rotations, and contrast adjustments may be applied on the fly to increase the diversity of the training data, and reduce overfitting. The data set will also be divided into training, validation, and test sets to ensure equal class balance between malignant, and benign classes.
	\item \textbf{Model Training, and Evaluation} \textemdash\ Multiple CNN models will be trained on the DDSM data set, constructed with TensorFlow, and Keras. The model will then be evaluated using metrics such as accuracy, precision, recall, and F1 score.
	\item \textbf{Explainability} \textemdash\ Grad-CAM will be used to overlay heat maps on input mammograms, demonstrating various regions that drive the model prediction.
\end{itemize}

\section{Technologies, and Methods}
In light of the previous components, mature, and well-supported libraries were selected to ensure that the system is easy to maintain, and extend.

\begin{itemize}
	\item \textbf{Data Handling} \textemdash\ Python as the fundamental language of choice for the entire stack, NumPy, and Pandas for data manipulation tasks, and OpenCV for image processing tasks.
	\item \textbf{Deep Learning} \textemdash\ TensorFlow for neural networks, with Keras front end for ease of use, and fast prototyping.
	\item \textbf{Hyperparameter Tuning} \textemdash\ Optuna for automated, and efficient hyperparameter tuning, and model selection.
    \item \textbf{Explainability} \textemdash\ Grad-CAM through \texttt{tf-keras-vis} to visualise the model's attention on the input image, and to highlight the areas of interest.
\end{itemize}

\section{Model}
The core of the system is a deep feature extractor powered by a convolutional neural network (CNN). Multiple methods will be explored, including alternative architectures like vision transformers (ViT), but initially, a custom CNN will be trialled. Alternative bases like VGG16, and ResNet50 will also be evaluated. This provides proven performance in breast image tasks, and manageable complexity \autocite{fatima2025application}. The uniform 16-layer architecture of VGG16 adapts well through ImageNet transfer learning, with previous work achieving high accuracy in the BreakHis histopathology data set by fine-tuning VGG16 \autocite{fatima2025application}. Additionally, VGG16 has shown excellent generalisation on unbalanced data \autocite{fatima2025application}, making it suitable for the class imbalance of the DDSM data set.

\section{Clinical Integration}
The system could be integrated into existing radiology workflows, providing a decision support tool that assists radiologists in identifying potential malignancies. The model's outputs, including the malignancy score and Grad-CAM heat maps, may be presented in the form of a user interface that is accessed by clinicians, and radiologists. Additionally, the system could log the model's predictions, and the corresponding images to create a database of cases that can be used for further research, and development. This would be facilitated by the use of a two-tier web application architecture, with a RESTful API for the model, and a front-end for user interaction. Requests to the API will return the model's predictions, and Grad-CAM heat maps, which can then be displayed in the front-end. As for the logging requests, a relational database such as PostgreSQL could be used to store the model's predictions, and the corresponding images, along with metadata such as the date, time, and user ID. This would allow for easy retrieval of cases for further analysis, and research. To set up the APIs, a web framework like FastAPI could be used, which is designed for building APIs quickly, and efficiently. FastAPI is built on top of Starlette, and Pydantic, and provides automatic generation of OpenAPI documentation, which can be used to document the API endpoints, and their parameters. The front-end could be built using a modern JavaScript framework like Next.js, which would allow for a responsive, and interactive user interface. The front-end would communicate with the backend API to retrieve the model's predictions, and Grad-CAM heat maps, which can then be displayed to the user. Support for asynchronous requests, and real-time updates could be implemented, which would allow for a more responsive user experience, and make the system able to scale. The front-end could also include features such as user authentication, and authorisation to ensure that only authorised users can access the system, and view logs. As for GPU acceleration, the model could be deployed on a cloud platform such as Google Cloud Platform (GCP), or Amazon Web Services (AWS), which provides CUDA-compatible GPU instances that can be used to accelerate model inference. This would allow for faster processing of images, and reduce the time taken to generate predictions. Additionally, the cloud platform could be used to store the model's weights, and configuration files, which can be easily accessed by the front-end, and backend APIs. In this pursuit, a series of MLOps pipelines could be developed in order to facilitate the training, and deployment of models, and reduce the time taken between prototyping, and production releases. Finally, horizontal scaling could be deployed through Kubernetes, which would allow for the system to handle a large number of requests, and scale up or down based on demand. However, the latter would mostly be important in cases with very high throughput that would likely not be seen within a single or even a few hospitals. This would probably be of significance should the system be deployed as a form of SaaS, where multiple hospitals around the globe make use of the service simultaneously. In this case, the system may also need to use message queues, such as RabbitMQ or Kafka, and be split into microservices to handle the high volume of requests, and ensure that the system remains responsive.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/architectural_diagram.png}
    \caption{Mock System Architecture}
    \label{fig:system_architecture}
\end{figure}

\noindent Figure \ref{fig:system_architecture} describes a mock system architecture that incorporates some of the elements described previously. It offers a simplified version of the system that would provide an adequate starting point for the project, and can be extended as needed. CI/CD pipelines are even included through the use of Jenkins, and GitHub where the \texttt{jenkinsfile} is hosted within the project's repoistory. Blue-green load balancing is also introduced to manage API versions within requests, and JWT for authentication. Amazon Aurora is used as a database that is PostgreSQL compatible.

\section{Plan of Evaluation}
The project as a whole will be evaluated using a variety of metrics. For the model itself, the most critical part of the system, the standard evaluation metrics of accuracy, precision, recall, and F1 score will be used to assess its performance on the DDSM data set, including the Area Under the Receiver Operating Characteristic Curve (AUC). Together with Grad-CAM heat maps, which show the model's attention on the input images, these metrics will provide a comprehensive outlook on the model's classification performance. The evaluation will focus on the model's performance, and the explainability of the outputs.

\end{document}
