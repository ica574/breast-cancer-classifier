{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac5be76-2cd6-4007-9d38-7fd19cb5c3ad",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca1fe35-afba-4f2f-ac3e-a57d93b7acba",
   "metadata": {},
   "source": [
    "The following cells include the essential importation of libraries, as well as helper functions that are used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea3d8be5-aefd-4c35-a417-563d9f984065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 18:25:30.881802: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-10 18:25:30.881832: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-10 18:25:30.881862: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-10 18:25:30.888199: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "/home/isaac/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so: undefined symbol: ncclCommRegister",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, ModelCheckpoint\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ViTImageProcessor\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     logging,\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     51\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/transformers/dependency_versions_check.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[1;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/transformers/utils/__init__.py:33\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     add_code_sample_docstrings,\n\u001b[1;32m     27\u001b[0m     add_end_docstrings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     replace_return_docstrings,\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     34\u001b[0m     ContextManagers,\n\u001b[1;32m     35\u001b[0m     ExplicitEnum,\n\u001b[1;32m     36\u001b[0m     ModelOutput,\n\u001b[1;32m     37\u001b[0m     PaddingStrategy,\n\u001b[1;32m     38\u001b[0m     TensorType,\n\u001b[1;32m     39\u001b[0m     add_model_info_to_auto_map,\n\u001b[1;32m     40\u001b[0m     cached_property,\n\u001b[1;32m     41\u001b[0m     can_return_loss,\n\u001b[1;32m     42\u001b[0m     expand_dims,\n\u001b[1;32m     43\u001b[0m     find_labels,\n\u001b[1;32m     44\u001b[0m     flatten_dict,\n\u001b[1;32m     45\u001b[0m     infer_framework,\n\u001b[1;32m     46\u001b[0m     is_jax_tensor,\n\u001b[1;32m     47\u001b[0m     is_numpy_array,\n\u001b[1;32m     48\u001b[0m     is_tensor,\n\u001b[1;32m     49\u001b[0m     is_tf_symbolic_tensor,\n\u001b[1;32m     50\u001b[0m     is_tf_tensor,\n\u001b[1;32m     51\u001b[0m     is_torch_device,\n\u001b[1;32m     52\u001b[0m     is_torch_dtype,\n\u001b[1;32m     53\u001b[0m     is_torch_tensor,\n\u001b[1;32m     54\u001b[0m     reshape,\n\u001b[1;32m     55\u001b[0m     squeeze,\n\u001b[1;32m     56\u001b[0m     strtobool,\n\u001b[1;32m     57\u001b[0m     tensor_size,\n\u001b[1;32m     58\u001b[0m     to_numpy,\n\u001b[1;32m     59\u001b[0m     to_py_obj,\n\u001b[1;32m     60\u001b[0m     transpose,\n\u001b[1;32m     61\u001b[0m     working_or_temp_dir,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     64\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[1;32m     65\u001b[0m     HF_MODULES_CACHE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m     try_to_load_from_cache,\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     94\u001b[0m     ACCELERATE_MIN_VERSION,\n\u001b[1;32m     95\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m     torch_only_method,\n\u001b[1;32m    201\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/transformers/utils/generic.py:442\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m--> 442\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_torch_pytree\u001b[39;00m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_model_output_flatten\u001b[39m(output: ModelOutput) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[Any], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_pytree.Context\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39mvalues()), \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/__init__.py:237\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    236\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: /home/isaac/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so: undefined symbol: ncclCommRegister"
     ]
    }
   ],
   "source": [
    "import os, gc, random\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler as MMS\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential, Input, Model\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    Add,\n",
    "    Activation,\n",
    "    ZeroPadding2D,\n",
    "    GlobalAveragePooling2D,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    AveragePooling2D,\n",
    "    MaxPooling2D,\n",
    "    GlobalMaxPooling2D,\n",
    "    LeakyReLU,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ReduceLROnPlateau,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.initializers import (\n",
    "    glorot_uniform,\n",
    "    he_uniform,\n",
    ")\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image as image_utils\n",
    "from tensorflow.keras.applications.imagenet_utils import (\n",
    "    preprocess_input,\n",
    "    decode_predictions,\n",
    "    preprocess_input,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.utils import get_file, plot_model, to_categorical, model_to_dot\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import ResNet50V2, MobileNetV2, VGG16\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib.patches import Circle\n",
    "import scipy as sp\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tf_explain.core.grad_cam import GradCAM\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import ViTImageProcessor\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "from transformers import ViTForImageClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce099e7-69a5-4520-922d-eb49448b14f0",
   "metadata": {},
   "source": [
    "This is a garbage collection function to clear as much memory as possible, and prevent the system from crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7bda47-4280-4996-abe2-32e2c02c7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_keras():\n",
    "    tensorflow.keras.backend.clear_session\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d4ae9-8f8d-41cc-8f78-0e3c78a7e2db",
   "metadata": {},
   "source": [
    "The function is called first to set the stage for the memory to be occupied as much as possible by the following tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df112953-fd4c-485c-9ef4-ca68437e09ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_keras()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56aa1e9-5310-4cc5-a4da-d9dc033ffbec",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1179cd62-8f2f-435e-91bc-47a9b0001629",
   "metadata": {},
   "source": [
    "This section focuses on loading, and preparing data for training, and testing from TFRecord files. A random seed is firstly set so that results are reproducibile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e614d6-fda7-45a0-9615-060808aa5760",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4286228c-1b90-469d-8e03-3038c07aa9e7",
   "metadata": {},
   "source": [
    "Two empty lists are initialised for images, and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db8514-fcf6-4611-bf90-6ec531e2b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c25ac43-cb8a-47ff-bed4-4fe1f948fc19",
   "metadata": {},
   "source": [
    "A dictionary is defined that specifies the structure of TFRecord data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3d3e20-a8de-4108-a709-d9497ed0d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "    \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"label_normal\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bb96fd-960b-492a-a75a-1d028f0d8e8b",
   "metadata": {},
   "source": [
    "A function is defined to process the TFRecord files, and store the results into the previously defined ```images```, and ```labels``` lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084a48c-36c2-49a1-b059-31f1727e5d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(files):\n",
    "    mydata = (\n",
    "        tf.data.TFRecordDataset(files, num_parallel_reads=4)\n",
    "        .shuffle(buffer_size=10000)\n",
    "        .cache()\n",
    "    )\n",
    "    mydata = mydata.map(\n",
    "        lambda x: tf.io.parse_example(x, features), num_parallel_calls=4\n",
    "    )\n",
    "    for image_features in mydata:\n",
    "        image = tf.io.decode_raw(image_features[\"image\"], tf.uint8)\n",
    "        image = tf.reshape(image, [299, 299])\n",
    "        image = np.asarray(image)\n",
    "        images.append(image)\n",
    "        labels.append(image_features[\"label\"].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a7b46-7504-4b64-9590-84d5dcc8b009",
   "metadata": {},
   "source": [
    "The garbage collection function is invoked before the files are parsed, to avoid any memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd2a09-dc6b-400b-b124-e659bf0060fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_keras()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d4f42c-cc21-4062-8c93-049b2adb8936",
   "metadata": {},
   "source": [
    "The target TFRecord files are defined in a list, prior to being processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1345d935-e771-4fab-9c64-28e081529ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    \"./data/training10_0/training10_0.tfrecords\",\n",
    "    \"./data/training10_1/training10_1.tfrecords\",\n",
    "    \"./data/training10_2/training10_2.tfrecords\",\n",
    "    \"./data/training10_3/training10_3.tfrecords\",\n",
    "    \"./data/training10_4/training10_4.tfrecords\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bfc02c-876d-4694-9a9a-633285171f23",
   "metadata": {},
   "source": [
    "The parsing function is called on the previously defined list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388d7e91-0187-419c-913c-ee17656abeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    read_file(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b5cec-c59c-4968-9983-b42bba3dedad",
   "metadata": {},
   "source": [
    "## Preparing Data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef9beea-bb72-4fa6-8342-0fee0daff0dd",
   "metadata": {},
   "source": [
    "This section will encompass cells that process the already loaded training data to make it more understandable to the CNN later in the notebook. Each image is first saved as a ```.jpg``` file in the ```train``` directory with enumerated filenames. The list of labels is saved as a CSV file within the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3139bf30-4529-4482-8a5b-4da4f26835d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "for image in images:\n",
    "    tmp = \"./train/\"\n",
    "    tmp += str(x)\n",
    "    tmp += \".jpg\"\n",
    "    plt.imsave(tmp, image)\n",
    "    x += 1\n",
    "np.savetxt(\"./train/labels.csv\", labels, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f48649-ac6e-40ad-9698-150ac6e999e5",
   "metadata": {},
   "source": [
    "The labels are loaded, and converted to boolean format, ideal for the binary classification task the CNN is to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6587451-721c-499f-8de4-c5c1365820fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.loadtxt(\"./train/labels.csv\", delimiter=\",\").astype(\"int\")\n",
    "y_train = np.where(y_train == 0, 0, y_train)\n",
    "y_train = np.where(y_train != 0, 1, y_train)\n",
    "print(np.bincount(y_train) / len(y_train))\n",
    "y_train = y_train.astype(\"str\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1d5bd0-eb47-4be3-b906-8ae88de76e82",
   "metadata": {},
   "source": [
    "The list of ```.jpg``` files from the ```train``` directory is retrieving using ```glob```, with the numeric part of the filename extracted, and storing in the ```mysplit``` list for further sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b217d8-712d-4a08-a8b3-25375da0cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "myf = glob.glob(\"./train/*.jpg\")\n",
    "mysplit = []\n",
    "\n",
    "for file in myf:\n",
    "    filename = file.split(\"/\")[-1]\n",
    "    number = int(filename.split(\".\")[0])\n",
    "    mysplit.append(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb94bc30-c5b7-4be2-8ac8-3ed070812728",
   "metadata": {},
   "source": [
    "A Pandas DataFrame is defined for training, with columns to store the image paths, and binary label, all sorted by their numeric value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed34595d-68c7-448b-9cf6-a6871387c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"file\": myf, \"number\": mysplit}\n",
    "traindf = pd.DataFrame(d)\n",
    "traindf = traindf.sort_values(by=[\"number\"])\n",
    "traindf[\"label\"] = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2de04-8284-46a1-a446-9ff86d1b1a6e",
   "metadata": {},
   "source": [
    "## Preparing Data for Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f38ffb-67b2-4e9e-b4a7-a82b8adc007d",
   "metadata": {},
   "source": [
    "This section applies methods similar to those seen within the previous section, but more suited towards preparing testing data. The following cell begins by loading test images, and labels from NumPy files, and concatenates them to form ```x_test``` for images, and ```y_test``` for labels. The ```y_test``` array is then converted to boolean format as was done for training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087cc1eb-e155-4770-9932-23d7dbd719b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.concatenate(\n",
    "    (\n",
    "        np.load(\"./data/cv10_data/cv10_data.npy\"),\n",
    "        np.load(\"./data/test10_data/test10_data.npy\"),\n",
    "    )\n",
    ")\n",
    "y_test = np.concatenate(\n",
    "    (np.load(\"./data/cv10_labels.npy\"), np.load(\"./data/test10_labels.npy\"))\n",
    ")\n",
    "y_test = np.where(y_test == 0, 0, y_test)\n",
    "y_test = np.where(y_test != 0, 1, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac54e7d7-a625-4ff5-8563-b3686eb0f89f",
   "metadata": {},
   "source": [
    "The testing data is then saved as ```.jpg``` files within the ```test``` directory with enumerated filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4528456-e66d-43d8-b41a-b94551a1991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "for i in range(x_test.shape[0]):\n",
    "    tmp = \"./test/\"\n",
    "    tmp += str(x)\n",
    "    tmp += \".jpg\"\n",
    "    plt.imsave(tmp, x_test[i, :, :, 0])\n",
    "    x += 1\n",
    "np.savetxt(\"./test/y_test.csv\", y_test, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced3ae8-7bf1-4469-998d-842fff6768bb",
   "metadata": {},
   "source": [
    "The test labels are then loaded from the CSV file, and casted as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b525781-fa33-48f5-95bc-afd71a7836f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.loadtxt(\"./test/y_test.csv\", delimiter=\",\").astype(\"int\")\n",
    "print(np.bincount(y_test) / len(y_test))\n",
    "y_test = y_test.astype(\"str\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3619ad1d-0975-4358-8d70-fdb838c974d7",
   "metadata": {},
   "source": [
    "Similar to the previous section, the filenames are retrieved as a list of ```.jpg``` files using ```glob```, with the numeric part of the filename extracted via ```glob```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf348c1-950d-48e6-8ac6-4eabeb99b001",
   "metadata": {},
   "outputs": [],
   "source": [
    "myf2 = glob.glob(\"./test/*.jpg\")\n",
    "mysplit = []\n",
    "\n",
    "for file in myf2:\n",
    "    filename = file.split(\"/\")[-1]\n",
    "    number = int(filename.split(\".\")[0])\n",
    "    mysplit.append(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1e187f-5feb-49a5-9094-ec394bc6b1b5",
   "metadata": {},
   "source": [
    "A Pandas DataFrame is created with this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a45c50-d1f8-4814-bb17-f9c23d915208",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"file\": myf2, \"number\": mysplit}\n",
    "testdf = pd.DataFrame(d)\n",
    "testdf = testdf.sort_values(by=[\"number\"])\n",
    "testdf[\"label\"] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfcc209-adf2-4d85-b535-4441ab33fede",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd60fa-c992-40ff-88fb-cbbbe1871f0f",
   "metadata": {},
   "source": [
    "The dataset is visualised within this section, with the garbage collection function being called first to alleviate the system's memory as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b52c2-48a9-4637-8f4d-59f025b2aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c0d5d9-9fb0-4a3a-a4d7-122690016842",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 9, figsize=[18, 3])\n",
    "fig2, ax2 = plt.subplots(1, 9, figsize=[18, 3])\n",
    "fig.suptitle(\"Positives Highlighted in Red/Orange\", fontsize=16)\n",
    "\n",
    "for i in range(9):\n",
    "    tmp = plt.imread(traindf.iloc[i, 0])\n",
    "    tmp = tmp[:, :, 0]\n",
    "    tmp2 = y_train[i].astype(\"int\")\n",
    "    ax[i].imshow(tmp) if tmp2 == 0 else ax[i].imshow(tmp, cmap=\"hot\")\n",
    "    fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "for i in range(9):\n",
    "    tmp = plt.imread(testdf.iloc[i, 0])\n",
    "    tmp = tmp[:, :, 0]\n",
    "    tmp2 = y_test[i].astype(\"int\")\n",
    "    ax2[i].imshow(tmp) if tmp2 == 0 else ax2[i].imshow(tmp, \"hot\")\n",
    "    fig2.subplots_adjust(wspace=0, hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e89a93e-a903-4c0f-b35b-bebc82956093",
   "metadata": {},
   "source": [
    "## Data Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9d9a76-828a-49ea-90c5-f8b553e6a472",
   "metadata": {},
   "source": [
    "The data processed in the previous steps is finally encapsulated into generators to feed it into the model during training, and testing. An ```ImageDataGenerator``` object is first defined to preprocess images by rescaling their pixel values. Each pixel is normalised into the range [0, 255] by being divided by 255. This is known to improve the model's training efficiency, and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01049af4-4b5c-488a-a789-a4bffea08dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aab76c-b8bf-4159-9683-47ab7bccc1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1.0 / 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3196206e-d890-4e8a-a2b6-3beb7d371131",
   "metadata": {},
   "source": [
    "The training generator yields batches of preprocessed training images, and their corresponding labels from the ```traindf``` DataFrame. This feeds the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3def7c41-4db6-4194-bda1-e7a51c8122d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=traindf,\n",
    "    directory=None,\n",
    "    x_col=\"file\",\n",
    "    y_col=\"label\",\n",
    "    class_mode=\"binary\",\n",
    "    color_mode=\"rgb\",\n",
    "    target_size=(299, 299),\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    subset=\"training\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12854d93-fbc6-4d5d-97cc-368f2585eb72",
   "metadata": {},
   "source": [
    "A similar generator is made for the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a9d67-8021-4d0f-80ff-aaa58facfb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = datagen.flow_from_dataframe(\n",
    "    testdf,\n",
    "    x_col=\"file\",\n",
    "    y_col=\"label\",\n",
    "    class_mode=\"binary\",\n",
    "    color_mode=\"rgb\",\n",
    "    target_size=(299, 299),\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba187d1a-c79d-44e6-8da2-3bc2e160a8bd",
   "metadata": {},
   "source": [
    "# ViT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6929d8af-438e-4f8d-be2c-d93489d0d8d5",
   "metadata": {},
   "source": [
    "The ViT model is trained slightly differently, owing to the fact that it utilised a PyTorch backend rather than TensorFlow through the HuggingFace ```transformers``` library. A base model is selected, and fine-tuned on the same dataset as the other models are. A seed is first set for the model training to be deterministic, and stay the same across multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2336116b-47c8-423b-94a5-50c47c42b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff49ad70-1f81-4853-b4b6-fcb9bed58af1",
   "metadata": {},
   "source": [
    "Firstly, the dataset is converted into a HuggingFace dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ce9e2-518d-4f40-b753-ba583c41c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(traindf[['file', 'label']])\n",
    "test_dataset = Dataset.from_pandas(testdf[['file', 'label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4872863-01e4-478d-9c3c-86629340d14b",
   "metadata": {},
   "source": [
    "A ViT image processor is then used to load the pre-trained transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a45a110-b1e5-4fd4-923f-2b94c847e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViTImageProcessor.from_pretrained('facebook/deit-tiny-patch16-224')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4defa7d1-0ff8-4211-8dd0-a82181ffb391",
   "metadata": {},
   "source": [
    "Data augmentation is performed on the training, and testing sets in order to help the model generalise better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4390a259-7645-400b-a0c6-ab1951109b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "])\n",
    "\n",
    "def transform(example_batch):\n",
    "    images = [augment(Image.open(f).convert(\"RGB\")) for f in example_batch[\"file\"]]\n",
    "    inputs = processor(images, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = [int(lbl) for lbl in example_batch[\"label\"]]\n",
    "    return inputs\n",
    "\n",
    "prepared_train_ds = train_dataset.with_transform(transform)\n",
    "prepared_test_ds = test_dataset.with_transform(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80377bde-a883-4b48-b11c-6331947f8d42",
   "metadata": {},
   "source": [
    "A collator is defined to produce batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e894699e-e44d-4ceb-90d7-85dcf41289ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
    "        \"labels\": torch.tensor([x[\"labels\"] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9fc0b9-8b64-4784-ad0d-797a18d3e105",
   "metadata": {},
   "source": [
    "A metric computing function is also defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d2f36-d8fc-4216-80d2-a37a5af20287",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(\n",
    "        predictions=np.argmax(p.predictions, axis=1),\n",
    "        references=p.label_ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae672ce-f13c-43a8-94f0-e41ef66c961f",
   "metadata": {},
   "source": [
    "The model is then defined for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190620f8-bc1f-4d21-b756-651fa85ae2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = [\"Negative\", \"Positive\"]\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'facebook/deit-tiny-patch16-224',\n",
    "    num_labels=len(labels_list),\n",
    "    id2label={str(i): c for i, c in enumerate(labels_list)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels_list)},\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ac4b8d-448a-4a3c-b8d6-046902f313a2",
   "metadata": {},
   "source": [
    "The trainer, with all the parameters for training the model, is defined, including the previously defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f265eef5-e733-41ea-be5c-69704450d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/vit-base-breast\",\n",
    "    per_device_train_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    dataloader_num_workers=4,\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,\n",
    "    save_steps=100,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to='tensorboard',\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_train_ds,\n",
    "    eval_dataset=prepared_test_ds,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c95f415-32a8-4f80-a0d8-cea2c50c4658",
   "metadata": {},
   "source": [
    "Training, and evaluation commences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455ac4a-ce5b-48f6-b8bf-2c1a16bac82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "metrics = trainer.evaluate(prepared_test_ds)\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e600932f-a17e-4267-aaa3-7bdb0f41465b",
   "metadata": {},
   "source": [
    "## Evaluating the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdff7257-854a-4595-ace4-e5aadefe723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model_tokenizer = ViTImageProcessor.from_pretrained(\"./models/vit-base-breast\")\n",
    "vit_model = ViTForImageClassification.from_pretrained(\"./models/vit-base-breast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0a19f-9aff-45d6-8fa1-f70577f55228",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_keras()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d0bfb8-556e-4ecf-97d0-8ff4b30c6daf",
   "metadata": {},
   "source": [
    "This section evaluates the previously trained models using standard metrics like accuracy, and loss on the training, and validation sets. In the following cell, these metrics are extracted into variables for later use from the performance of the model across different epochs during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b9252-fb60-4add-bdde-16d9ce70e8d5",
   "metadata": {},
   "source": [
    "Predictions are made on the test set are made in order to generate a final classification report, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a227ca86-f6d4-4d12-8c64-8e84e250514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "vit_predictions = trainer.predict(prepared_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d12ca7b-3914-4ce5-83d5-ff67835a8598",
   "metadata": {},
   "source": [
    "The previously-made predictions, and actual values are gathered, and fed into ```scikit-learn``` to generate the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0873d68-36ca-46ef-b4c5-c9732edcba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_predictions = np.argmax(vit_predictions.predictions, axis=1)\n",
    "vit_act=y_test.astype('int')\n",
    "vit_cm = confusion_matrix(vit_act, vit_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b446a-f3ce-4104-8d13-b3a1f3c60621",
   "metadata": {},
   "source": [
    "The results are then plotted using ```seaborn```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2162391-9e6b-4603-9f4d-8f8d64e45e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(\n",
    "    vit_cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=target, yticklabels=target\n",
    ")\n",
    "plt.title(\"Confusion Matrix - ViT\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58f0709-128c-4298-ad5b-ac5943bc1cc0",
   "metadata": {},
   "source": [
    "The same values are used to generate the final classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d6402-065c-496a-90a3-d81556a56e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_report_dict = classification_report(vit_act, vit_predictions, target_names=[\"Negative\", \"Positive\"], output_dict=True)\n",
    "vit_df = pd.DataFrame(vit_report_dict).transpose()\n",
    "vit_df.style.background_gradient(cmap=\"Blues\").format(\"{:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5b3f73-c63e-40ad-8f3a-bf55bbeeb871",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(vit_act, vit_predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic - ViT')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5efe5b-e677-4ef3-a5a2-8bde234ad336",
   "metadata": {},
   "source": [
    "# Model Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc98a2ca-292c-4e50-9eb7-22d2665efb1c",
   "metadata": {},
   "source": [
    "This final section touches upon XAI or explainable AI, by implementing _Grad-CAM_ through the ```tf-explain``` module. This provides a visual indication of the parts of the image that influenced the classification made of the model, explaining the results achieved. This is done for the first ten images within the testing set, but can easily be done for any other image as well. The following function accepts a path to an image, the model used, and it's final layer, and returns the explained version of the image, and the classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd62151c-dafe-420c-b55e-d00b4a948be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_base = VGG16(weights=\"imagenet\", include_top=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "fig.suptitle(\"Grad-CAM Visualizations for Test Images 1-10 - VGG16\", fontsize=18)\n",
    "\n",
    "for i in range(1, 11):\n",
    "    img = tf.keras.preprocessing.image.load_img(f\"./test/{i}.jpg\", target_size=(224, 224))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.keras.applications.vgg16.preprocess_input(img_array)\n",
    "    input_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    preds = vgg16_base.predict(input_array)\n",
    "    predicted_class = np.argmax(preds[0])\n",
    "\n",
    "    explainer = GradCAM()\n",
    "    grid = explainer.explain(\n",
    "        validation_data=(input_array, None),\n",
    "        model=vgg16_base,\n",
    "        class_index=predicted_class,\n",
    "        layer_name=\"block5_conv3\",\n",
    "    )\n",
    "\n",
    "    row = (i - 1) // 5\n",
    "    col = (i - 1) % 5\n",
    "    ax = axes[row, col]\n",
    "    ax.imshow(grid)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Img {i} - Class {predicted_class}\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c82de7-01e3-47c3-a0a4-eda09253acac",
   "metadata": {},
   "source": [
    "In the above plots, warmer colours indicate higher influence on the model's predicted class, and cooler colours the opposite."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
